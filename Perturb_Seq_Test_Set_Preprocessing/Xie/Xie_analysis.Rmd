---
title: "Counts_Data"
output: html_document
date: "2023-12-26"
---

```{r setup, include=FALSE}
library(reticulate)

# Use Sherlock’s Python module path — verified working path
use_python("/share/software/user/open/python/3.9.0/bin/python3", required = TRUE)

knitr::opts_chunk$set(echo = TRUE)
```


# About

This R markdown file imports the raw counts data for the Xie paper from [GSE129826](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE129837)

```{r, message=FALSE}
library(Seurat)
library(dplyr)
library(Matrix)
library(hdf5r)
library(readxl)
library(GenomicRanges)
library(data.table)
library(readr)
library(rtracklayer)
```

# Download the data

`wget -O data/GSE129826_RAW.tar "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE129826&format=file"`
`tar -xf data/GSE129826_RAW.tar -C data`
`gunzip data/*.txt.gz`

# Filter UMIs according to Xie
```{python}
import sys
import os
import glob

# Add the sgrnaprocessing module to path
sys.path.append("Global-analysis-K562-enhancers/VirtualFacs/sgrnaprocessing")
from correct_sgrna import _load_data, _filter_umi

# Extract sorting key from filename
def extract_batch_numbers(filename):
    start = filename.find('Batch') + len('Batch')
    return filename[start+1:start+4]

# Locate the .txt.gz files in the 'data/' directory
txt_files = sorted(
    glob.glob('data/*.txt'),
    key=extract_batch_numbers
)

# Output to archive/filtered_UMIs
output_dir = "data/filtered_UMIs"
os.makedirs(output_dir, exist_ok=True)

# Process and save
for idx, file_path in enumerate(txt_files):
    print(f"processing {idx}: {file_path}")
    
    df = _load_data(file_path)
    df = _filter_umi(df)
    
    df_non_zero_count = (df >= 1).astype(int)
    avg_column_sum = df_non_zero_count.sum(axis=0).mean()
    print(f"Avg col sum: {avg_column_sum}")
    
    filename = os.path.basename(file_path).replace(".gz", "")
    save_path = os.path.join(output_dir, filename)
    df.to_csv(save_path)
    print(f"Saved: {save_path}")
```


# Import

```{r}
# Function to sort the files once imported
extract_batch_numbers <- function(filename) {
    # Find the start position of 'Batch' in the filename
    start <- regexpr("Batch", filename) + nchar("Batch")
    
    # Extract the next three characters after 'Batch'
    substring(filename, start + 1, start + 3)
}
```

## Get the file paths

Let's import all the h5 files

```{r}
file_path = "data"
h5_files <- list.files(path = file_path, pattern = "*.h5", full.names = TRUE)
h5_files <- h5_files[order(sapply(h5_files, extract_batch_numbers))]
```

Let's import all the filtered UMIs

```{r}
file_path = "data/filtered_UMIs"
umi_files <- list.files(path = file_path, pattern = "*.txt", full.names = TRUE)
umi_files <- umi_files[order(sapply(umi_files, extract_batch_numbers))]
```

## Read into Seurat Objects and Normalize

```{r}
# Normalization function as per the provided formula
normalize_meta_cell <- function(seurat_obj) {
  # Extract the raw counts matrix
  raw_counts <- GetAssayData(seurat_obj, slot = "counts")
  total_reads <- sum(raw_counts)
  
  # Compute the meta cell
  meta_cell_cpm = (Matrix::rowSums(raw_counts) + 1) / total_reads * 1e6
  
  # CPM normalize the raw_counts matrix
  normalized_counts = t(t(raw_counts)/colSums(raw_counts)) * 1e6
  # Divide each CPM normalized cell by the CPM normalized meta-cell
  normalized_counts = normalized_counts / meta_cell_cpm
  
  # Create a new data slot in the Seurat object for the normalized data
  seurat_obj[['normalized']] <- CreateAssayObject(counts = normalized_counts)
  
  return(seurat_obj)
}
```

```{r}
h5_to_seurat <- function(file_names = h5_files, meta_cell_norm = FALSE) {
  
  seurat_objects <- list()

  for (file in file_names) {
      # Message for progress
      print(paste0("Processing: ", file))
    
      # Read the data
      data <- Read10X_h5(file, use.names=FALSE, unique.features=FALSE)

      # Filter out PAR_Y genes
      par_y_genes <- grep("_PAR_Y", rownames(data), value = TRUE)
      data <- data[!rownames(data) %in% par_y_genes, ]

      # Create a Seurat object
      seurat_object <- CreateSeuratObject(counts = data)
      
      # Perform Meta-cell normalization (if required)
      if (meta_cell_norm) {
        seurat_object <- normalize_meta_cell(seurat_object)
      }
      
      # Store in list
      seurat_objects[[basename(file)]] <- seurat_object
  }
  
  return(seurat_objects)
}

seurat_objects <- h5_to_seurat(file_names = h5_files, meta_cell_norm = FALSE)
```

## Add UMIs as Assay

For each seurat object in `seurat_objects` we need to get the intersection of cell barcodes between the UMI matrices and the seurat_objects.
Then we need add the UMI matrix to the seurat object as an assay

```{r}
import_and_format_UMI_matrix <- function(file_path) {
  
  # Check if file is gzipped and read accordingly
  if(grepl("\\.gz$", file_path)) {
    # For gzipped files
    df <- read.csv(gzfile(file_path))
  } else {
    # For regular files
    df <- read.csv(file_path)
  }
  
  # Set the row names and remove the first column
  rownames(df) <- df$X
  df <- df[,-1]
  
  # Create a matrix out of the text file
  UMI_matrix <- as.matrix(df)
  
  # Return the formatted UMI_matrix
  return(UMI_matrix)
}

get_common_barcodes <- function(seurat_object, UMI_matrix) {
  
  # Get the barcodes from the seurat object
  seurat_barcodes <- colnames(seurat_object)
  # Removing "-1" from each element of the seurat barcodes
  seurat_barcodes <- sub("-1$", "", seurat_barcodes)
  
  # Get the barcodes from the UMI matrix
  umi_barcodes <- colnames(UMI_matrix)
  
  # Intersect the barcodes from the seurat object and the UMI matrix
  common_barcodes <- intersect(seurat_barcodes, umi_barcodes)
  
  # Print number of barcodes removed
  cat(paste0("Length of Seurat Barcodes: ", length(seurat_barcodes), 
               "\nLength of UMI Barcodes: ", length(umi_barcodes),
               "\nLength of Common Barcodes: ", length(common_barcodes), "\n"))
  
  # Return the intersected barcodes
  return(common_barcodes)
}

filter_seurat_and_UMI <- function(common_barcodes, seurat_object, UMI_matrix) {
  
  # Filter UMI matrix
  UMI_matrix <- UMI_matrix[, common_barcodes]
  colnames(UMI_matrix) <- paste0(colnames(UMI_matrix), "-1")
  
  # Filter seurat object
  common_barcodes <- paste0(common_barcodes, "-1")
  seurat_object <- seurat_object[, common_barcodes]
  
  # Return both as a list
  return(list(seurat_object = seurat_object, UMI_matrix = UMI_matrix))
}

add_UMI_matrix_as_assay <- function(seurat_objects, umi_files) {
  
  for (i in seq_along(umi_files)) {
    # Print message
    print(paste0("Processing: ", umi_files[[i]]))
    
    # Import and format the UMI matrix from the file path
    UMI_matrix <- import_and_format_UMI_matrix(umi_files[[i]])
    
    # Get the intersected barcodes from the seurat object and the UMI matrix
    common_barcodes <- get_common_barcodes(seurat_objects[[i]], UMI_matrix)
    
    # Filter the seurat and UMI matrices given the common barcodes
    results <- filter_seurat_and_UMI(common_barcodes, seurat_objects[[i]], UMI_matrix)
    
    # Add UMI matrix to seurat object as an assay
    results$seurat_object[["UMI_matrix"]] <- CreateAssayObject(counts = results$UMI_matrix)
    seurat_objects[[i]] <- results$seurat_object
  }
  
  # Return the seurat objects that have the perturb status assay
  return(seurat_objects)
}

# Run the function
seurat_objects <- add_UMI_matrix_as_assay(seurat_objects, umi_files)
```

# Get Guide Info

## Format BLAT input file

```{r}
# We want to input a list of protospacers
create_BLAT_input <- function(sequence_list, output_path) {
  
  # We will create a file in fasta format with the name of the gRNA and the seq + NGG sequence
  fasta_file <- paste0(">", seq_along(sequence_list), "\n", sequence_list, "NGG")
  
  # Save the concatenated fasta formatted sequences into a file
  writeLines(paste(fasta_file, collapse = "\n"), output_path)
}
```

```{r}
union_row_names <- Reduce(union, lapply(seurat_objects, function(x) rownames(x[["UMI_matrix"]])))

create_BLAT_input(sequence_list = union_row_names, output_path = "to_be_BLAT.fa")
```

To use BLAT, we have to set up a server, and run it on our own. We can do that following [this link](https://genome.ucsc.edu/FAQ/FAQblat.html).

We want to input the output of BLAT and get (chr, start, end, name, strand, spacer)

## Format BLAT output file

```{r}
format_BLAT_results <- function(BLAT_output_path) {

  # Skip the first 5 lines to start reading from the actual data (they contain the header)
  blat_results <- read.delim(BLAT_output_path, header = FALSE, sep = "\t", skip = 5)
  
  # Create a vector of column names and assign to df
  column_names <- c("match", "mis-match", "rep.match", "N's", "Q gap count", 
                    "Q gap bases", "T gap count", "T gap bases", "strand", "Q name", 
                    "Q size", "Q start", "Q end", "T name", "T size", 
                    "T start", "T end", "block count", "blockSizes","qStarts", "tStarts")
  colnames(blat_results) <- column_names
  
  return(blat_results)
}

clean_blat_results <- function(sequence_list, blat_results) {
  
  # Note how many sequences weren't returned by blat_results
  original.length <- length(sequence_list)
  blat.length <- length(unique(blat_results$`Q name`))
  print(paste0(original.length - blat.length, " sequences didn't return a BLAT hit"))
  
  # Remove all rows with alt|random|fix chromosomes in T name
  rows.to.remove <- grep("alt|random|fix", blat_results$`T name`, ignore.case = TRUE)
  blat_results <- blat_results[-rows.to.remove, ]
  print(paste0("Removed ", length(rows.to.remove), " rows with alt|random|fix chromosomes"))

  # Remove all rows that don't have the max block size
  rows.to.remove <- blat_results$`blockSizes` < max(blat_results$`blockSizes`)
  blat_results <- blat_results[!rows.to.remove, ]
  print(paste0("Removed ", sum(rows.to.remove), " rows with < max(blockSizes)"))

  # Remove all rows that have a mismatch
  rows.to.remove <- blat_results$`mis-match` > 0
  blat_results <- blat_results[!rows.to.remove, ]
  print(paste0("Removed ", sum(rows.to.remove), " rows that have >0 mismatches"))

  # Print all rows that have duplicates after filtering
  q.names.w.duplicates <- unique(blat_results[duplicated(blat_results$`Q name`),]$`Q name`)
  print(paste0("There are ", length(q.names.w.duplicates), " duplicate names after filtering"))

  # If there are still duplicates, print the lines from blat_results
  if (length(q.names.w.duplicates) > 0) {
    print(blat_results[blat_results$`Q name` %in% q.names.w.duplicates, ])
  }

  # And remove those lines from the result
  rows.w.more.than.one.hit <- blat_results$`Q name` %in% q.names.w.duplicates
  blat_results <- blat_results[!rows.w.more.than.one.hit, ]
  print(paste0("Removed ", sum(rows.w.more.than.one.hit), " rows of spacers that had more than one hit"))

  # Return the filtered blat_results
  return(blat_results)
}

format_guide_info <- function(sequence_list, filtered_blat_results) {
  
  # Let's return the columns we need for the guide_targets file
  guide_info <- filtered_blat_results[ , c("T name", "T start", "T end", "Q name", "strand")]
  colnames(guide_info) <- c("chr", "start", "end", "name", "strand")
  
  # For each row, add the spacer to a spacer column that's in the `Q name`th position
  guide_info$spacer <- sapply(as.integer(guide_info$name), function(q) sequence_list[q])
  
  # Rename each guide to be chr:start-end_strand
  guide_info$name <- paste0(guide_info$chr, ":", 
                            guide_info$start, "-", 
                            guide_info$end, "_", 
                            guide_info$strand)

  
  return(guide_info)
}

create_guide_info_from_BLAT <- function(sequence_list, BLAT_output_path) {
  
  # Format BLAT output file
  blat_results <- format_BLAT_results(BLAT_output_path)
  
  # Clean the blat results
  filtered_blat_results <- clean_blat_results(sequence_list, blat_results)

  # Format guide_info from the filtered blat results
  guide_info <- format_guide_info(sequence_list, filtered_blat_results)

  # # Return a dataframe with the results
  return(guide_info)
}

guide_info <- create_guide_info_from_BLAT(sequence_list = union_row_names,
                                          BLAT_output_path = "out.psl")
```

## Align guides to cCREs

```{r}
format_cCRE_file <- function(cCRE_file_path) {
  # Import the bed file of DHS sites
  candidate_cres <- read.table(cCRE_file_path, 
                               header=FALSE, 
                               sep="\t", 
                               stringsAsFactors=FALSE, 
                               quote="")
  
  # Keep the first four columns
  candidate_cres <- candidate_cres[,c(1:4)]
  
  # Rename the columns
  colnames(candidate_cres) <- c("target_chr", "target_start", "target_end", "target_name")
  
  return(candidate_cres)
}


create_granges <- function(guide_info, candidate_cres) {
  
  # Create the GRanges object for the guides
  guides_gr <- makeGRangesFromDataFrame(
    guide_info[,c("chr", "start", "end")], 
    starts.in.df.are.0based = TRUE)
  
  # Create the GRanges object for the target cCREs
  targets_gr <- makeGRangesFromDataFrame(
    candidate_cres[,c("target_chr", "target_start", "target_end")], 
    starts.in.df.are.0based = TRUE)
  
  # Return a list of both objects
  return(list(guides_gr = guides_gr, targets_gr = targets_gr))
}


overlap_guides_with_cCREs <- function(granges_objects) {
  
  # Overlap the GRanges objects
  overlaps <- findOverlaps(query = granges_objects$guides_gr, 
                           subject = granges_objects$targets_gr, 
                           ignore.strand = TRUE)
  
  return(overlaps)
}

create_final_table <- function(candidate_cres, guide_info, overlaps) {
  
  # Merge the guides that overlap with a cCRE with information about that cCRE
  guide_targets <- bind_cols(
    guide_info[queryHits(overlaps), ], 
    candidate_cres[subjectHits(overlaps), ]
    )
  
  # Add missing columns
  guide_targets$target_strand <- "."
  guide_targets$target_type <- "enh"
  
  # Reorder the columns
  guide_targets <- guide_targets[,c("chr", "start", "end", "name", "strand", "spacer", "target_chr", "target_start", "target_end", "target_name", "target_strand", "target_type")]
  
  return(guide_targets)
}

create_guide_targets <- function(guide_info, cCRE_file_path) {
  
  # Format the candidate cis-regulatory elements file
  candidate_cres <- format_cCRE_file(cCRE_file_path)
  
  # Create the GRanges objects
  granges_objects <- create_granges(guide_info, candidate_cres)
  
  # Overlap the guides with the target file
  overlaps <- overlap_guides_with_cCREs(granges_objects)
  
  # Create the final guide_targets table
  guide_targets <- create_final_table(candidate_cres, guide_info, overlaps)
  
  return(guide_targets)
}

guide_targets <- create_guide_targets(guide_info = guide_info, cCRE_file_path = "../candidate_cre_data/sample1_candidate_cres.bed")
```

# Merge Seurat Objects

```{r}
merge_seurat_objects <- function(seurat_objects) {
  
  # Add unique identifiers to each Seurat object first
  for (i in 1:length(seurat_objects)) {
    # Create a unique label for each object
    cell_id_label <- paste0("Batch", i)

    # Rename cells to include this unique label
    # This avoids the duplication of batch labels in cell names during merging
    seurat_objects[[i]] <- RenameCells(
      seurat_objects[[i]], new.names = paste0(cell_id_label, "_", Cells(seurat_objects[[i]]))
      )
    
    # Add batch information as metadata
    seurat_objects[[i]]$batch <- rep(cell_id_label, ncol(seurat_objects[[i]]))
  }

  # Now merge all Seurat objects
  merged_seurat <- seurat_objects[[1]]
  for (i in 2:length(seurat_objects)) {
    # Merge without using add.cell.ids since cells already have unique identifiers
    merged_seurat <- merge(merged_seurat, y = seurat_objects[[i]])
  }

  print("Merging completed.")
  return(merged_seurat)
}

merged_seurat <- merge_seurat_objects(seurat_objects)
```

# Save Files for CRISPRi Pipeline

## Counts Matrix

```{r}
# Extract the count matrix from the Seurat object
count_matrix <- GetAssayData(object = JoinLayers(merged_seurat), layer = "counts")

# Remove the decimal expansion from row names (gene symbols)
cleaned_gene_symbols <- gsub("\\..*$", "", rownames(count_matrix))

# Convert the matrix to a data.table
dt <- as.data.table(as.matrix(count_matrix))

# Add gene symbols as the first column named 'VECTOR'
dt[, VECTOR := cleaned_gene_symbols]

# Move VECTOR to the first column
setcolorder(dt, c("VECTOR", names(dt)[-which(names(dt) == "VECTOR")]))

# Write the data.table to a file
dir.create("results", recursive = TRUE, showWarnings = FALSE)
fwrite(dt, file = "results/dge.txt.gz", sep = "\t", quote = FALSE)
rm(dt)
gc()
```

## Perturb Status

for each perturb status row name, need to get the corresponding name from the guide targets matrix

```{r}
# Extract matrix
data_matrix <- as.matrix(GetAssayData(object = merged_seurat@assays$UMI_matrix, layer = "counts"))

# Convert matrix to data frame and add row names as "VECTOR" column without the spacer
df <- as.data.frame(data_matrix)

# Find the index in guide_targets that matches each row name in df
matched_indices <- match(rownames(df), guide_targets$spacer)

# Identify rows in df that have NA in matched_indices
rows_to_remove <- is.na(matched_indices)

# Remove rows that have NA matched indices
df <- df[!rows_to_remove, ]

# Update matched_indices to exclude NAs
matched_indices <- matched_indices[!rows_to_remove]

# Replace row names with corresponding 'name' values from guide_targets
rownames(df) <- guide_targets$name[matched_indices]

# Remove spacer names
df$VECTOR <- rownames(df)

# Exclude the last column (VECTOR) and then prepend it to the front
df <- df[, c("VECTOR", colnames(df)[-length(colnames(df))])]

# Save to file
fwrite(df, file = "results/perturb_status.txt.gz", sep = "\t", quote = FALSE)
rm(df)
gc()
```

## Guide Targets

```{r}
# Save the guide targets file
write_tsv(guide_targets, "results/guide_targets.tsv")
```

## Metadata

```{r}
# Create a tibble/data.frame with the necessary columns
metadata_df <- tibble(
  cell_barcode = colnames(merged_seurat),
  # Extract everything up to the first underscore
  cell_batches = sub("_.*", "", colnames(merged_seurat))
)

# Save to a gzipped TSV file
write.table(metadata_df,
            gzfile("results/metadata.tsv.gz"),
            sep = '\t',
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE)
```
