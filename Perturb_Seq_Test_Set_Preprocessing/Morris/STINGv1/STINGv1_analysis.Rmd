---
title: "STINGv1_Analysis"
output: html_document
date: "2023-09-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About

This file is for downloading and re-analyzing the data from the STING-Seq v1 dataset in the Morris et al. 2023 paper: "Discovery of target genes and pathways at GWAS loci by pooled single-cell CRISPR screens". The data was retrieved from [GSE171452](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE171452).

# Getting Data

## Import Libraries

```{r, message=FALSE}
# # For getting data
library(GEOquery)
# # For formatting data
library(Matrix)
# # Performing analysis
library(Seurat)
# # Reading in supplements
library(readxl)
# # API for rsID information
library(httr)
library(jsonlite)
# # Add a progress bar to rsID GET *Loading this after dplyr sometimes doesn't work*
library(plyr)
# # Running API on df and formatting
library(dplyr)
library(tidyr)
# # Import to compare overlaps of DHS and guides
library(GenomicRanges)
# # Converting the Genomic Ranges file to a bed file
# library(rtracklayer)
# # Write table to tsv file
library(readr)
```

## Retrieve list of File Names

```{r}
# This is for STING-seq v1 only
cDNA <- getGEO("GSM5225857")
HTO <- getGEO("GSM5225858")
GDO <- getGEO("GSM5225859")
```

## Download the files and read them into objects

```{r}
# Function to download and read files
download_and_read_file <- function(obj) {
  # Create an empty list to hold data frames or matrices
  data_list <- list()

  # Create "data" directory if it does not exist
  if (!dir.exists("data")) {
    dir.create("data")
  }

  # Get all header names that start with 'supplementary_file'
  supplementary_keys <- names(obj@header)[startsWith(names(obj@header), "supplementary_file")]

  # Loop through each supplementary file
  for (key in supplementary_keys) {
    # Get supplementary file link
    url <- obj@header[[key]]

    # Extract filename from URL
    filename <- basename(url)
    filepath <- file.path("data", filename)  # store in "data" subdirectory

    # Check if the file already exists in the "data" directory
    if (!file.exists(filepath)) {
      download.file(url, filepath)
    } else {
      message(paste("File", filename, "already exists. Skipping download."))
    }

    # Strip away .gz and then find the actual file extension
    filename_no_gz <- tools::file_path_sans_ext(filename)
    file_ext <- tools::file_ext(filename_no_gz)
    
    # Determine the file type based on its extension and then read it
    if (file_ext == "mtx") {
      data <- readMM(gzfile(filepath))
    } else if (file_ext == "csv") {
      data <- read.table(gzfile(filepath), header = FALSE, sep = ",")
    } else if (file_ext == "tsv") {
      data <- read.table(gzfile(filepath), header = FALSE, sep = "\t")
    } else {
      data <- NULL
    }

    # Add to the list with a variable name based on the filename
    variable_name <- tools::file_path_sans_ext(tools::file_path_sans_ext(filename)) # Remove double extension
    data_list[[variable_name]] <- data
  }

  return(data_list)
}

# Example usage
# Assume cDNA, HTO, and GDO are already populated
cDNA_data <- download_and_read_file(cDNA)
HTO_data <- download_and_read_file(HTO)
GDO_data <- download_and_read_file(GDO)

# Now cDNA_data, HTO_data, and GDO_data are lists containing data frames or matrices
# corresponding to each supplementary file.
# Create a function to retrieve those files:
# Identify the files in each list by keyword
get_file_by_keyword <- function(file_list, keyword) {
  return(file_list[[grep(keyword, names(file_list))]])
}
```

# Seurat Filtering

## Create File Objects

### Retrieve each dataset

```{r}
# Retrieve each cDNA file
cDNA_matrix <- get_file_by_keyword(cDNA_data, "matrix")
cDNA_barcode <- get_file_by_keyword(cDNA_data, "barcode")
cDNA_feature <- get_file_by_keyword(cDNA_data, "feature")

# Add col and row to matrix
rownames(cDNA_matrix) <- cDNA_feature$V1
colnames(cDNA_matrix) <- cDNA_barcode$V1



# Retrieve each HTO file
HTO_matrix <- get_file_by_keyword(HTO_data, "matrix")
HTO_barcode <- get_file_by_keyword(HTO_data, "barcode")
HTO_feature <- get_file_by_keyword(HTO_data, "feature")

# Subset each HTO file to remove all Gene rows
HTO_matrix <- HTO_matrix[-(1:36601),]
HTO_feature <- HTO_feature[-(1:36601),]
# Add col and row to matrix
rownames(HTO_matrix) <- HTO_feature$V1
colnames(HTO_matrix) <- HTO_barcode$V1



# Retrieve each GDO file
GDO_matrix <- get_file_by_keyword(GDO_data, "matrix")
GDO_barcode <- get_file_by_keyword(GDO_data, "barcode")
GDO_feature <- get_file_by_keyword(GDO_data, "feature")

# Subset each GDO file to remove all Gene rows
GDO_matrix <- GDO_matrix[-(1:36601),]
GDO_feature <- GDO_feature[-(1:36601),]
# Add col and row to matrix
rownames(GDO_matrix) <- GDO_feature$V1
colnames(GDO_matrix) <- GDO_barcode$V1
```

### Intersect the Cell Barcodes of each Dataset

```{r}
# Find the intersect of all the barcodes
common_cells <- intersect(intersect(cDNA_barcode$V1, HTO_barcode$V1), GDO_barcode$V1)

# Subset each matrix and barcode table with these common cells
cDNA_matrix <- cDNA_matrix[,common_cells]
HTO_matrix <- HTO_matrix[,common_cells]
GDO_matrix <- GDO_matrix[,common_cells]

# Subset barcodes to keep only rows where V1 is in common_cells
cDNA_barcode <- cDNA_barcode[cDNA_barcode$V1 %in% common_cells, ,drop=FALSE]
HTO_barcode <- HTO_barcode[HTO_barcode$V1 %in% common_cells, ,drop=FALSE]
GDO_barcode <- GDO_barcode[GDO_barcode$V1 %in% common_cells, ,drop=FALSE]

# Set the rownames for each of the barcode dataframes
rownames(cDNA_barcode) <- cDNA_barcode$V1
rownames(HTO_barcode) <- HTO_barcode$V1
rownames(GDO_barcode) <- GDO_barcode$V1

# Check that everything matches up
all(colnames(cDNA_matrix) == cDNA_barcode$V1)
all(colnames(HTO_matrix) == HTO_barcode$V1)
all(colnames(GDO_matrix) == GDO_barcode$V1)
```

### Retrieve the GDO UMI file

```{r}
# Get the UMI file
GDO_UMI <- get_file_by_keyword(GDO_data, "umi")

# Make the first row the header then delete the first row
colnames(GDO_UMI) <- GDO_UMI[1,]
GDO_UMI <- GDO_UMI[-1,]

# Reset the row numbers 2-211 -> 1-210
rownames(GDO_UMI) <- NULL
```

## Create the Seurat Object

### Modifying cDNA row names for MT QC compatibility

```{r}
# Make the rownames of the cDNA matrix the unique gene symbols
# Most of the genes are already unique, but some are not (row 3236,3238)
rownames(cDNA_matrix) <- make.unique(cDNA_feature$V2)

# Store these unique gene symbols in another column in the feature table
cDNA_feature$V4 <- make.unique(cDNA_feature$V2)
```

### Create the Object and Add GDO/HTO Assays

```{r}
# Create Seurat object
cDNA_seurat <- CreateSeuratObject(counts = cDNA_matrix,
                                  project = "cDNA",
                                  meta.data = cDNA_barcode)

# Add the HTO Assay
cDNA_seurat[["HTO"]] <- CreateAssayObject(counts = HTO_matrix)

# Add the GDO Assay
cDNA_seurat[["GDO"]] <- CreateAssayObject(counts = GDO_matrix)
```

## cDNA filtering

"For STING-seq v1, we processed cDNA UMI count matrices and retained cells between the 15th to 99th percentiles for unique gene count, between the 20th and 99th percentiles for total cDNA UMI count, and between the 5th and 90th percentile for mitochondrial percentage." (Morris et al. 2023)

```{r}
# Identify mitochondrial genes in your feature data (please adjust the identifier based on your actual gene names)
cDNA_seurat[["percent.mt"]] <- PercentageFeatureSet(cDNA_seurat, pattern = "^MT-")

# Plot the features after filtering
VlnPlot(cDNA_seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```

## HTO Filtering

### Demultiplex the HTO

"Next, we center-log-ratio (CLR) transformed the HTO UMI counts and demultiplexed cells by their transformed HTO counts to identify singlets. We used the HTODemux function implemented in Seurat v.4.0.0 to maximize the number of singlets detected." (Morris et al. 2023)

Using the following [vignette](https://satijalab.org/seurat/articles/hashing_vignette) as reference, I demultiplex cells based off their cell hash.

```{r}
# Normalize HTO data, here we use centered log-ratio (CLR) transformation
cDNA_seurat <- NormalizeData(cDNA_seurat, assay = "HTO", normalization.method = "CLR")

# Assign single cells by to their origin (demultiplex)
cDNA_seurat <- HTODemux(cDNA_seurat, assay = "HTO", positive.quantile = 0.99)
```

### Visualize the HTO Demux

```{r}
# We can visualize the results
table(cDNA_seurat$HTO_classification.global)

# Group cells based on the max HTO signal
Idents(cDNA_seurat) <- "HTO_maxID"
RidgePlot(cDNA_seurat, assay = "HTO", features = rownames(cDNA_seurat[["HTO"]])[1:4], ncol = 2)

# Visualize the RNA count by HTO classification
Idents(cDNA_seurat) <- "HTO_classification.global"
VlnPlot(cDNA_seurat, features = "nCount_RNA", pt.size = 0.1, log = TRUE)
```

## Sub-setting

"For STING-seq v1, we processed cDNA UMI count matrices and retained cells between the 15th to 99th percentiles for unique gene count, between the 20th and 99th percentiles for total cDNA UMI count, and between the 5th and 90th percen tile for mitochondrial percentage. Next, we center-log-ratio (CLR) transformed the HTO UMI counts and demultiplexed cells by their transformed HTO counts to identify singlets. We used the HTODemux function implemented in Seurat v.4.0.0 to maximize the number of singlets detected. We used then processed the GDO UMI count matrix, keeping cells between the 1st and 99th percentiles for total GDO count and used 10x Cell Ranger predicted GDO thresholds per cell, but required at least three UMIs per GDO to assign a GDO to a given cell. This resulted in a high-confidence set of 7667 single cells for the STING-seq v1 experiment." (Morris et al. 2023)

```{r}
# Let's filter based on all the parameters stated through the notebook that are in the methods of Morris et al. 2023
cDNA_seurat_filtered <- subset(cDNA_seurat, 
                               subset = 
                                 nFeature_RNA >= quantile(cDNA_seurat$nFeature_RNA, 0.15) &
                                 nFeature_RNA <= quantile(cDNA_seurat$nFeature_RNA, 0.99) &
                                 nCount_RNA >= quantile(cDNA_seurat$nCount_RNA, 0.20) &
                                 nCount_RNA <= quantile(cDNA_seurat$nCount_RNA, 0.99) &
                                 percent.mt >= quantile(cDNA_seurat$percent.mt, 0.05) &
                                 percent.mt <= quantile(cDNA_seurat$percent.mt, 0.90) &
                                 cDNA_seurat$HTO_classification.global == 'Singlet' &
                                 nCount_GDO >= quantile(cDNA_seurat$nCount_GDO, 0.01) &
                                 nCount_GDO <= quantile(cDNA_seurat$nCount_GDO, 0.99)
                               )
```

## Checking

```{r}
# Group cells based on the max HTO signal
Idents(cDNA_seurat_filtered) <- "HTO_maxID"
RidgePlot(cDNA_seurat_filtered, assay = "HTO", features = rownames(cDNA_seurat[["HTO"]])[1:4], ncol = 2)

# Visualize the RNA count by HTO classification
Idents(cDNA_seurat_filtered) <- "HTO_classification.global"
VlnPlot(cDNA_seurat_filtered, features = "nCount_RNA", pt.size = 0.1, log = TRUE)

# Create a histogram of UMIs specifically (nCount_RNA)
UMIs <- cDNA_seurat_filtered@meta.data$nCount_RNA
hist(UMIs, 
     breaks=50, 
     main="Histogram of UMIs per cell", 
     xlab="Number of UMIs", 
     col="skyblue", 
     border="black")


# Visualize basic features
VlnPlot(cDNA_seurat_filtered, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```

# Guide Mapping

For the pipeline, genomic coordinates are needed. Because only the protospacer and the rsid of the SNP were provided, we need to map these to the genome

## BLAT

### Get protospacer/cell metadata information

```{r}
# Path to the excel file of insterest
excel_file_path <- "../supplementary_tables/science.adh7699_table_s3.xlsx"

# This file has our protospacer info for each gRNA along with the SNP name
guide_info <- read_excel(excel_file_path, sheet = 1)
# Set the column names to be the second row and take out those rows afterwards
colnames(guide_info) <- guide_info[2,]
guide_info <- guide_info[-(1:2),]
# Only interested in v1
guide_info <- guide_info[guide_info$`gRNA Library` == "v1",]
guide_info$`gRNA ID` <- rownames(cDNA_seurat@assays$GDO)


# This file has all the cell barcodes kept after filtering
sc_metadata <- read_excel(excel_file_path, sheet = 3)
# Set the column names to be the second row and take out those rows afterwards
colnames(sc_metadata) <- sc_metadata[2,]
sc_metadata <- sc_metadata[-(1:2),]
# Only interested in v1
sc_metadata <- sc_metadata[sc_metadata$Library == "v1",]
# Remove all columns specific to v2 (ADT, Lane)
sc_metadata <- sc_metadata[, -c(2, 9, 11, 12, 13, 14)]
```

### Add a Column in `guide_info` for sgRNA + NGG

```{r}
# We will BLAT sgRNA + NGG
guide_info$`BLAT sequence` <- sprintf("%sNGG", guide_info$`sgRNA guide sequence`)
```

### Running BLAT with API

```{r}
# # Function to perform BLAT search
# perform_blat_search <- function(seq, query_type = "dna", database = "hg38") {
#   base_url <- "https://genome.ucsc.edu/cgi-bin/hgBlat"
#   params <- list(userSeq = seq, type = query_type, db = database, output = "json")
#   resp <- httr::GET(url = base_url, query = params)
#   if (httr::status_code(resp) != 200)
#     stop("BLAT API error: ", httr::status_code(resp))
#   jsonlite::fromJSON(httr::content(resp, "text"))
# }
# 
# # Query every sgRNA sequence
# blat_results <- lapply(guide_info$`BLAT sequence`, perform_blat_search)

# The above code was commented out, and the resulting file `blat_results` can be found in the resources folder
blat_results <- readRDS("blat_results.rds")
```

## Convert rsID to Genomic Coordinates

### Create function to use rsID API

We can use [this API](https://clinicaltables.nlm.nih.gov/apidoc/snps/v3/doc.html) to confirm that the rsID coordinate matches up with the coordinates returned by BLAT. The data for this API is taken from [dbSNP](https://www.ncbi.nlm.nih.gov/snp/) from the NIH

```{r}
# Create a function to interact with the API that returns rsID information
fetch_snp_info_clinicaltables <- function(rsid) {
  base_url <- "https://clinicaltables.nlm.nih.gov/api/snps/v3/search"
  params <- list(
    terms = rsid,
    df = "rsNum,38.chr,38.pos,38.alleles,38.gene",
    sf = "rsNum,38.chr,38.pos,38.alleles,38.gene",
    maxList = 1
  )
  
  resp <- GET(url = base_url, query = params)
  if (status_code(resp) != 200) {
    return(paste("Error:", status_code(resp)))
  }
  
  data <- fromJSON(content(resp, "text"))
  
  if (length(data[[2]]) == 0) {
    return(paste("No data found for", rsid))
  }

  # Retrieve the row of data
  info <- data[[4]][1,]
  
  # Check for length mismatch
  if (length(info) == length(c("rsNum", "Chromosome", "Position", "Alleles", "Gene"))) {
    # Convert to a named list for better readability
    info <- setNames(as.list(info), c("rsNum", "Chromosome", "Position", "Alleles", "Gene"))
  } else {
    print("Length mismatch between info and expected names.")
    return(NULL)
  }
  
  return(info)
}
```

### Run GET rsID function on all rsIDs in v1

```{r}
# Initialize empty lists to store chromosomal and position information
chromosome_list <- list()
position_list <- list()
rsid_list <- guide_info$Target

# Initialize the progress bar
progress <- create_progress_bar("text")
progress$init(length(rsid_list))

for (i in 1:length(rsid_list)) {
  progress$step()  # Update the progress bar
  
  rsid <- rsid_list[i]
  
  # Check if the target is an rsID or chr:position_allele
  if (startsWith(rsid, "rs")) {
    info <- fetch_snp_info_clinicaltables(rsid)
  
    # Check if the info is NULL
    if (is.null(info)) {
      print(paste("Skipping", rsid, "because info is NULL."))
      chromosome_list[[i]] <- NA
      position_list[[i]] <- NA
      next
    }
  
    # Check if info contains "No data found"
    if (is.character(info) && grepl("No data found", info)) {
      print(paste("Skipping", rsid, "because no data is available."))
      chromosome_list[[i]] <- NA
      position_list[[i]] <- NA
      next
    }
  
    # Extract Chromosome and Position information
    chromosome <- info$Chromosome
    position <- info$Position
  
    if (!is.null(chromosome) && !is.null(position)) {
      chromosome_list[[i]] <- chromosome
      position_list[[i]] <- position
    } else {
      print(paste("Error while fetching Chromosome or Position for", rsid))
      chromosome_list[[i]] <- NA
      position_list[[i]] <- NA
    }
  } else {
    gRNA_ID <- guide_info$`gRNA ID`[i]
    # Check if gRNA_ID starts with 'SNP'
    if (startsWith(gRNA_ID, "SNP")) {
      # Extract Chromosome and Position from Target
      split_target <- strsplit(rsid, "[:_]")[[1]]
      chromosome_list[[i]] <- split_target[1]
      position_list[[i]] <- split_target[2]
    } else {
      # If gRNA_ID does not start with 'SNP', set both to NA
      print(paste("Skipping", rsid, "because gRNA_ID does not start with 'SNP'."))
      chromosome_list[[i]] <- NA
      position_list[[i]] <- NA
    }
  }
}
```

## Add all information to `guide_info`

### Add rsID results to `guide_info`

```{r}
# Convert lists to vectors and add them as new columns to guide_info
guide_info$rsID_Chr <- unlist(chromosome_list)
guide_info$rsID_pos <- unlist(position_list)
```

### Add BLAT results to `guide_info`

#### Create the `blat_results_df`

Some of the BLAT JSON results returned 2 dimensional vectors because there were multiple hits. In this case, I took the hit where the chromosome *did not* have the word "alt" or "fix" in it. If neither of those words were in the chromosome, I took the hit where the chromosome matched the rsID chromosome

```{r}
# Get the column names from the first sample's fields
if (!is.null(blat_results[[1]]$fields)) {
  initial_fields <- blat_results[[1]]$fields
} else {
  stop("The first sample does not have fields.")
}

# Initialize an empty dataframe with the required columns
blat_results_df <- data.frame(matrix(ncol = length(initial_fields), nrow = 0))
colnames(blat_results_df) <- initial_fields

# Loop through each guide_info and blat_results
for (i in 1:length(guide_info$`gRNA ID`)) {
  
  x <- blat_results[[i]]
  
  # Initialize a new row to NA
  new_row <- as.data.frame(matrix(data = NA, nrow = 1, ncol = length(initial_fields)))
  colnames(new_row) <- colnames(blat_results_df)
  
  # If x is not NULL, and $fields and $blat exist
  if (!is.null(x) && !is.null(x$blat) && (length(x$blat) != 0)) {
    
    # Initialize to NULL as we haven't picked a row yet
    row_to_use <- NULL
    
    # Check if it is two-dimensional or more
    if (dim(x$blat)[1] >= 1) {
      
      # First condition: Eliminate rows that have "alt" or "fix" in the chromosome name
      rows_without_alt_or_fix <- !grepl("alt|fix", x$blat[, 14])
      
      # If there are rows remaining, proceed
      if (any(rows_without_alt_or_fix)) {
        
        # Second condition: Pick the row matching the guide_info Chromosome
        expected_chromosome <- paste0("Chr", guide_info$rsID_Chr[i])
        rows_with_expected_chromosome <- x$blat[, 14] == expected_chromosome
        
        # If any row matches the expected chromosome, use it; otherwise use the first non-"alt" and non-"fix" row
        if (any(rows_with_expected_chromosome)) {
          row_to_use <- which(rows_with_expected_chromosome)[1]
        } else {
          row_to_use <- which(rows_without_alt_or_fix)[1]
        }
      } else {
        # If all rows contain "alt" or "fix", just pick the first one
        row_to_use <- 1
      }
    } else {
      # If x$blat is one-dimensional, row_to_use should be 1
      row_to_use <- 1
    }
    
    if (!is.null(row_to_use)) {
      # Populate the new row based on the selected row
      new_row[] <- x$blat[row_to_use, ]
    }
  }
  
  # Add this row to the blat_results_df
  blat_results_df <- rbind(blat_results_df, new_row)
}
```

#### Concatenate `guide_info` and `blat_results_df`

```{r}
all_guide_info <- cbind(guide_info, blat_results_df)

# Add a percent match column
# We add 1 because the BLAT sequence as an `N` value, which will not match
all_guide_info$percent_match <- (as.integer(all_guide_info$match) + 1) / as.integer(all_guide_info$qSize)
```

## Check Guide Mapping

```{r}
for (i in seq_len(nrow(all_guide_info))) {
  row = all_guide_info[i, ]
  gRNA_ID <- row["gRNA ID"]
  matches <- row["matches"]
  rsID_Chr <- row["rsID_Chr"]
  percent_match <- row["percent_match"]
  target <- row["Target"]
  
  if (grepl("^SNP-\\d+-\\d+$", gRNA_ID)) {
    # Check to see if BLAT worked
    if (is.na(matches)) {
      print(paste(gRNA_ID, ": Warning: SNP did not return BLAT match"))
      # If BLAT worked, make sure the percent_match is 100%
    } else if (percent_match != 1) {
      print(paste(gRNA_ID, ": Warning: SNP did not match 100%"))
    }
    # Check to see if rsID match worked
    if (is.na(rsID_Chr)) {
      print(paste(gRNA_ID, ": Warning: SNP rsID did not match"))
    }
    # Make sure the non targetting guide didn't get mapped
  } else if (target == "nt") {
    if (!is.na(matches)) {
      print(paste(gRNA_ID, ": Warning: Non-target returned BLAT match"))
    }
    # make sure all positive controls mapped
  } else {
    if (is.na(matches)) {
      print(paste(gRNA_ID, ": Warning: Positive Control did not map"))
    }
  }
}
```

In summary:
-   There were a few rsIDs that didn't get mapped. In these cases, the genomic coordinates were picked up from BLAT.
-   There were a few gRNA guide sequences that didn't align to the genome with BLAT.

# Guide Matching with ABC CREs

In the ABC pipeline, we use DNase-seq data to define cCREs. We want to make sure that the gRNAs in the Morris et al. 2023 paper are aligned to the CREs specified in our DNase-seq pipeline. For instance, perhaps two elements defined in the Morris paper lie on the same element in our DNase-seq definition of a cCRE. We import the file with all DNase peaks as `sample1_candidate_cres.bed`.

## Combining `all_guide_info` with `sample1_candidate_cres.bed`

Now that we have a bed file of all locations in the genome where there's a DHS, we align our guides with these DHS peaks.

### Import the bed file

```{r}
# Import the bed file of DHS sites
candidate_cres <- read.table("../../candidate_cre_data/sample1_candidate_cres.bed", header=FALSE, sep="\t", stringsAsFactors=FALSE, quote="")
# Keep the first four columns
candidate_cres <- candidate_cres[,c(1:4)]
# Rename the columns
colnames(candidate_cres) <- c("target_chr", "target_start", "target_end", "target_name")
```

### Subset the `all_guide_info` dataframe

Here I subset the dataframe with all the guide information, and I remove any rows with NA values.

-   This includes the 2 guides, which didn't map via BLAT.

-   This doesn't include the guides which didn't map via their rsID value as I didn't use the returned rsID information for the final dataframe, and those guides returned BLAT hits.

-   This includes the non-targeting guides as they do not have genomic coordinates and they aren't explicitly used in the downstream power analysis.

I also remove the positive control guides, which are targeting the TSS.

For the protospacer, I used the BLAT sequence, which is the guide sequence + NGG.

```{r}
# Remove the two guides that didn't return hits
guides_to_be_merged <- all_guide_info %>% 
  filter(!(startsWith(`gRNA ID`, "SNP") & is.na(matches)) &
         `gRNA ID` %in% rownames(cDNA_seurat_filtered@assays$GDO) &
           !startsWith(`gRNA ID`, "NTC-"))


# Subset the columns needed
guides_to_be_merged <- guides_to_be_merged[,c("tName", "tStart", "tEnd", "gRNA ID", "strand", "BLAT sequence")]

# Change the column names
colnames(guides_to_be_merged) <- c("chr", "start", "end", "name", "strand", "spacer")
```

### Create the GRanges Objects

```{r}
guides_gr <- makeGRangesFromDataFrame(
  guides_to_be_merged[,c("chr", "start", "end")], 
  starts.in.df.are.0based = TRUE)

targets_gr <- makeGRangesFromDataFrame(
  candidate_cres[,c("target_chr", "target_start", "target_end")], 
  starts.in.df.are.0based = TRUE)
```

### Find overlaps of guides and DHSs

Some of the guides didn't overlap with DHS.
-   Specifically, there are 115 hits from 174 queries (176 total SNP guides minus the 2, which did not have BLAT hits). This means that 59 guides did not qualify as targeting a DHS.

```{r}
overlaps <- findOverlaps(query = guides_gr, subject = targets_gr, ignore.strand = TRUE)
guide_targets <- bind_cols(
  guides_to_be_merged[queryHits(overlaps), ], 
  candidate_cres[subjectHits(overlaps), ]
  )
```


### Format the final table

The input for the power analysis requires two more columns `target_strand` and `target_type`. We also need to reset the row numbers

```{r}
# Reset row numbers
rownames(guide_targets) <- NULL
# We ignored strand when doing target search so we can just use `.` as the target_strand
guide_targets$target_strand <- "."

# Initiate the column with default value "TSSCtrl"
guide_targets$target_type <- "TSSCtrl"
# Then, for rows where the `name` starts with "SNP-", set the value to "enh"
guide_targets$target_type[startsWith(guide_targets$name, "SNP-")] <- "enh"

# Change the TSSCtrl Target Names so that the snakemake pipeline recognizes TSSCtrls as hits 
guide_targets_TSS_target_name <- guide_targets %>%
  mutate(target_name = 
           ifelse(target_type == "TSSCtrl", 
                  cDNA_feature$V1[match(stringr::str_extract(name, "^[^-]+"),cDNA_feature$V2)],
                  target_name))
```

# Prepare Data

## Set all Filepaths

```{r}
guide_targets_wo_nt_file_path <- "results/guide_targets.tsv"
counts_matrix_file_path <- 'results/dge.txt.gz'
perturb_status_file_path <- 'results/perturb_status.txt.gz'

# Create the results directory if needed
if (!exists("results/")) {dir.create("results/")}
```

## Guide Information

We want to save the `guide_targets` table into a .tsv

### Save the guide targets table without NT

```{r}
# Save the guide targets without NT controls
write_tsv(guide_targets_TSS_target_name, guide_targets_wo_nt_file_path)
```

## Counts Matrix

We want to get a txt.gz file of Genes x Cell Barcode

```{r}
data_matrix <- as.matrix(LayerData(object = cDNA_seurat_filtered, layer = "counts"))

# Convert matrix to data frame and add row names as "VECTOR" column
df <- as.data.frame(data_matrix)
df$VECTOR <- rownames(data_matrix)

# Rearrange the columns to have VECTOR as the first column
df <- df[, c("VECTOR", colnames(data_matrix))]

# Replace the names of each gene symbol with its associated ENSEMBL ID
# Create a named vector for easy replacement
replace_vector <- setNames(cDNA_feature$V1, cDNA_feature$V4)

# Replace values in df$VECTOR based on the replace_vector
df$VECTOR <- replace_vector[df$VECTOR]


write.table(df, 
            gzfile(counts_matrix_file_path), 
            sep = '\t', 
            row.names = FALSE, 
            col.names = TRUE, 
            quote = FALSE)
```

## Perturb Status

We want a binary txt.gz file of gRNA x Cell Barcode

```{r}
# Extract matrix
data_matrix <- as.matrix(LayerData(object = cDNA_seurat_filtered@assays$GDO, layer = "counts"))

# Convert matrix to data frame and add row names as "VECTOR" column
df <- as.data.frame(data_matrix)
df$VECTOR <- rownames(data_matrix)

# Rearrange the columns to have VECTOR as the first column
df <- df[, c("VECTOR", colnames(data_matrix))]

# Save to file
write.table(df,
            gzfile(perturb_status_file_path),
            sep = '\t', 
            row.names = FALSE,
            col.names = TRUE, 
            quote = FALSE)
```
